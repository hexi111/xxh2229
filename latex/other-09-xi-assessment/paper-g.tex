%\doublespacing
\newcommand{\BH}{\mbox{~~~~}}
\newcommand{\SH}{\mbox{~~}}

\newcommand{\E}[1]{\footnote{{\color{red}\em  some error}}}
\newcommand{\GVL}[1]{{\color{red}\em  GVL: #1}~}
\newcommand{\WANG}[1]{{\color{blue}\em  WANG: #1}~}
\newcommand{\YOUNGE}[1]{{\color{blue}\em  YOUNGE: #1}~}

\newcommand{\AUTHOR}{Xi He\\
%\newcommand{\AUTHOR}{Xi He, Gregor von Laszewski, Lizhe Wang and Andrew Younge\\
%\small Service Oriented Cyberinfrastructure Lab\\
\small GCCIS, Rochester Institute of Technology, Rochester, NY 14623\\
\small e-mail: hexi111@gmail.com
%\small e-mail: \{hexi111, laszewski, Lizhe.Wang, ajy4490\}@gmail.com
\vspace{-1cm}%
}
\newcommand{\TITLE}{Thermal-aware Resource Management Framework}

%\TABLEOFCONTENTS

\title{\TITLE}
\author{\AUTHOR}

\maketitle



\GVL{please do not delete comments if they are not yet addressed}

\GVL{note there are still problems there that i dod not comment on, but they can be fixed later, e.g. grammar.}

\begin{abstract}
Large energy consumption in data centers has become a challenging problem with the emergence of cloud computing and large scale data centers. In this paper, we present an architectural framework for thermal-aware resource management while considering energy efficiency. The framework consists of a layered architecture and integrates 
an easy-to-use set of client tools and a thermal-aware task management middleware to schedule tasks based on thermal conditions within a cluster and among different data centers. As part of this paper we focus on the development of a thermal-aware task scheduling component for a single data center. This component is fundamental to our future activities, while considering to balance the temperature distribution in a single data centers, thus implicitly minimizing energy cost in data centers. 
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In recent years, many large scale data centers have been deployed with high density computing clusters and server farms to support high performance scientific applications. However, besides the scientific challenges, many operational issues in data centers need to be addressed. One of those issues is the large energy consumption in data centers. According to U.S. Environmental Protection Agency (EPA), 61 billion kilowatt-hours of power was consumed in data center in 2006, that is 1.5 percent of all US electricity consumption costing around \$4.5 billion \cite{www-epa}. In fact, the energy consumption in data centers doubled between 2000 and 2006 and EPA estimates that the energy usage will double again by 2011. 

Much effort has been spend towards addressing the power crisis in data centers. For example, large amount of research work has been conducted on smart cooling techniques \cite{patel2002tcc, beitelmal2007tfp, moore2005dcw, moore2006wao}  to improve energy-efficiency in data centers. In \cite{hsu2005fap,DBLP:conf/sc/HsuF05} dynamic voltage and frequency scaling (DVFS) are proposed to reduce processor power dissipation with modest performance loss. Server virtualization \cite{DBLP:conf/sosp/BarhamDFHHHN03} is another way to address the large power consumption problem. Most servers and desktops today are in use only 5-15\% of the time they are powered on, yet most x86 hardware consumes 60-90\% of the normal workload power even when idle. With virtualization technique, all the VMs share the same physical resources and the utilization of the physical server can reach up to 80\% with ignorable overhead \cite{DBLP:conf/nca/SilvaASTA07}. Thus, although using virtualization results typically in a performance loss using the available hardware more efficiently provides great contribution in regards to the utilization. In addition, a wide variety of task scheduling algorithms are reported to minimize the energy consumption in data centers \cite{DBLP:conf/ccgrid/KimBK07,DBLP:conf/sc/GeFC05,DBLP:conf/cluster/TangGV07,DBLP:conf/usenix/MooreCRS05,DBLP:conf/aPcsac/VandersterBD07}. 

Temperature is considered as an important physical metrics in data centers \cite{hamann2008mat}. First, efficient thermal management can decrease the cooling costs in data centers. For example, in \cite{DBLP:conf/usenix/MooreCRS05} it is pointed out that a data center that can run the same computational workload and cooling configuration, but maintain an ambient room temperature that is 5$^\circ$C cooler through intelligent thermal management, can lower computer room air conditioning (CRAC) power consumption by 20\%-40\% for a \$1-\$3 million savings in annual cooling costs. Second, efficient thermal management can also increase hardware reliability. Some studies \cite{DBLP:conf/fast/AndersonDR03, cole2000edr} show that a 15$^\circ$C rise increases hard disk drive failure rates by a factor of two. 

A recent research work \cite{DBLP:conf/cluster/TangGV07} indicates that computing nodes' temperature distribution will affect the energy cost of cooling system in data centers. It also shows that minimizing the maximal temperature of all the nodes will minimize the cooling energy cost of a data center. Assume a set of tasks consume a fixed amount of energy in data centers, an appropriate temperature distribution of computing nodes will reduce the maximal temperature of all the nodes and thus significantly conserve cooling energy consumption. There exists many available metrics for measuring temperature distribution. In this paper we employ standard deviation of computing nodes' temperature as our metrics.  

\begin{comment}
In this paper we use standard deviation \cite{www-sd} to describe computing nodes' thermal distribution. Computing nodes' temperature standard deviation is the measure of how the computing nodes' temperature is close to their average value. A low standard deviation indicates that every computing node' temperature value tends to very close to the average value. A high standard deviation indicated that the temperature values in the computing nodes are distributed in a large range. Therefore, assume the same energy in data centers is needed to execute the same set of tasks, the goal of the efficient thermal management is to balance the thermal distribution on each node and make the computing nodes' temperature standard deviation as low as possible so that         
\end{comment}

In our study, we present a thermal-aware resource management framework (TARMF) for managing resource in a energy efficient fashion. As part of our long term research effort, we are currently focusing on thermal-aware task scheduling, which is an important component in TARMF. The contribution of this paper is two-fold: first, we present a novel framework to solve resource management problem. Second, we developed a thermal-aware task scheduling for data centers, which will save amount of cooling energy cost.

The reminder of this paper is organized as follows. First we introduce our thermal-aware resource management framework and the motivation for this framework in Section 2,3. Then we spent the rest of the sections discussing  thermal-aware task scheduling, which is an important component in thermal-aware resource management framework. We review related works and present an example to illustrate our thermal-aware task scheduling in Section 4 and 5. In Section 6, 7, 8 we model data centers and their components, define our problem and present our solution to the problem.

%%%%%%%%%%%%
\section{Motivation}
%%%%%%%%%%%%%%
\GVL{not a real motivation, mixing design with modtivation, possibly needs requirements section.}

Currently computing resource management is an important and complex research topic and involves a wide variety of issues ranging from resource organization, resource discovery, resource access to resource integration and security. These issues are dependent on specific users needs and roles. Thus requirements posed by resource owners may be different form those posed by resource users. For example, resource owners need to consider resource organization issues while resource end users deal with performance related resource access issues. In recent years energy efficiency is becoming another essential factor that resource management have to take into account. 

To address the mentioned problem, a multi-layer architecture is required to isolate problem into various independent and transparent layers. Each layer has its responsibility and communicate with other layers through predefined interfaces. For example, there should be a resource layer that organizes different resources and provides resource access interface to other layers. This layer is also responsible for adopting the latest technique to improve resource and energy efficiency.

%%%%%%%%%%%%
\section{Requirements}\label{S:req}
%%%%%%%%%%%%%%


%%%%%%%%%%%%
\section{Thermal-aware resource management framework}
%%%%%%%%%%%%%%

To address our requirments we are designing a thermal-aware resource management framework (TARMF). As this framework is quite extensive we have chosen a layered architecture design that allows us to gradually expand upon it and implement it in phases.
In Figure \ref{F:t3} we show our TARMF while focusing on the  3-layer architecture design with a resource, mediator and client layer.  This design addresses explicitly the requirements that we have identified earlier \ref{S:req}. We describe the functionality of each layer next.

\begin {itemize}
\item The {\em resource layer} contains a number of advanced cyberinfrastructure resources. For our research we focus on clusters that are housed in data centers. It also contains a service that maps tasks onto a cluster while considering thermal properties.

\item The {\em client layer} is composed of a set of easy-to-use client tools designed to facilitate the use of computing resources without putting much burden on the end user. Howeevr it is important to note that we provide a variety of different interfaces for different endusers. Programmers are able to use APIs, administrators are exposed through scripts, and all of them can in addition use graphical user interfaces to interact with the TARMF. This helps especially those that may just use TARMF as a monitoring tool. 

\item The {\em mediator layer} provides all services that establish the connection between the client and the resource layer. Important features of this layer include: the secure connection between clients an resources, the availability of a super scheduling service to distribute tasks not only on one data center, but accross data centers while considering thermal properties. Furthermore, the mediator layer must have a sophisticated workflow engine included that allows interfacing with Grids and clouds easily and provides scripting capabilities.

\end {itemize}

\FIGURE{htb}
 {images/figure5.png}
 {1}
 {The 3-layered architecture of our TARMF.}
 {F:t3}


%\GVL{ next part needs some cleanup, but this can be done later}
%\GVL{mix of components, modules, frameworks, need to be tightened and properly used}

%Components in different layers communicating securely with the help of secure Web services.

The most typical and important computing resources in the {\em resource layer} are data centers which house computer system, storage system and other associated components and provide powerful computing ability for task execution. Each data center is equipped with a thermal-aware task scheduling component, all of which are integrated into an overarching thermal scheduling component that are together integrated in our framework controlled, monitored, and accessed through the mediator layer. In accordance to prior work in Grid computing on meta scheduling \cite{DBLP:conf/mg/HeilgeistSR08} we also refer to it as {\em thermal-aware meta scheduler}.

Besides the {\em thermal-aware meta scheduler}, the {\em mediator layer} also provides such functionality as task organization, security and client interface API. We have designed a sophisticated mediator service Cyberaide Shell \cite{las09ccgrid}. %\GVL{ shel is just a portion of the mediator in my view}
 Cyberaide Shell provides a set of services such as task management service, authentication and authority service, task submission service to allow users to get access to complex cyberinfrastructure using its command line interface. Cyberaide Shell also exposes its functionality to a wide variety of frameworks and programming languages so that other tools in the client layers can be developed based on Cyberaide Shell.   
%\FIGURE{htb}
% {images/figure9.png}
% {0.95}
% {The architecture of Cyberaide Portal}
% {F:t9}
 
The client layer is targeted to offer users who are	 unfamiliar with the complex Grid and data center infrastructure, a tool to decrease the learning curve of using sophisticated computing resources across different data centers. 
To further assist in providing access to non computer scientists, we have also developed a Cyberaide Portal \cite{las08-javascript} framework that allows accessing data centers through a web browser with a sophisticated window based user interface.%As shown in Figure \ref{F:t9},
 Cyberaide Portal has a layered architecture and integrates Web Services, Javascript and other Web 2.0 technology. We are working on adding new components for auditing and monitoring thermal-aware task scheduling across data centers.
%\footnote{This work was done mainly by Fugang Wang, however, the mediator is going to be used in this research activity, demonstrating the generality of our architecture and proving its usefulness also in another activity}. 

%\GVL{ I think the java script framework should be included in the figure. I would also mention that new components will be made available for auditing and monitoring thermal aware task scheduling across data centers.}

%In the next sections, we will focus on thermal aware task scheduling.

%%%%%%%%%%%%
\section{Related Work}
%%%%%%%%%%%%%%

Thermal-aware task scheduling \cite{DBLP:conf/dac/MukherjeeMM05, DBLP:conf/iccad/JayaseelanM08,DBLP:conf/date/CoskunRW07,DBLP:conf/vlsid/JayaseelanM09,arani2007ota,DBLP:conf/ispass/YangZCZJ08} has been one of hot and important research topics in the area of embedded systems because efficient power management is critical in those system in which there is no dedicated\footnote{word?} energy source.
Recently, with the emergence of cloud computing and the development of large scale data centers, a number of power-aware task scheduling \cite{DBLP:conf/ccgrid/KimBK07,DBLP:conf/sc/GeFC05} and thermal-aware task scheduling \cite{DBLP:conf/cluster/TangGV07,DBLP:conf/usenix/MooreCRS05,DBLP:conf/aPcsac/VandersterBD07} have been presented.

In \cite{DBLP:conf/ccgrid/KimBK07,DBLP:conf/sc/GeFC05},  power reduction is achieved by the power-aware task scheduling on DVS-enabled commodity systems which can adjust the supply voltage and support multiple operating points. In \cite{DBLP:conf/cluster/TangGV07,DBLP:conf/usenix/MooreCRS05} thermodynamic formulation of steady state hot spots and cold spots in data centers is examined and based on the formulation several task scheduling algorithms are presented to reduce the cooling energy consumption. In \cite{DBLP:conf/aPcsac/VandersterBD07} the researchers study the task characteristics and temperature profiles for a subset of SPEC'2K benchmarks and exploit these profiles to suggest several scheduling algorithms aimed at achieving lower cluster temperature.

In our work, we study different types of task temperature profiles and the relationship between temperature distribution and cooling energy cost in data centers, then we develop our novel task scheduling based on our study.
 
%%%%%%%%%%%%%
\section{A Motivational Example}
%%%%%%%%%%%%%%%

Given certain compute processor and steady ambient  temperature, a task-temperature profile is the temperature increase along with the task execution. 
It has been observed that different types of computing tasks generate different amount of heat, 
therefore featuring with distinct task-temperature profiles~\cite{DBLP:conf/aPcsac/VandersterBD07}. 

\FIGURE{!h}
 {images/Buffalo_data}
  {1.0}
 {Task-temperature profiles in buffalo data center}
 {F:buffalo}

\FIGURE{!h}
 {images/crafty.pdf}
  {1.0}
 { Task-temperature profile of SPEC2000 (crafty)}
 {F:crafty}
 

Task-temperature profile can be obtained by using some profiling tools. 
As task-temperature is related to processor type and ambient environment and normally profiling tools 
can only get task-termperture in a standard environment, 
the task-temperature is also termed as general task-temperature profile. 
Figure \ref{F:buffalo} shows an overall task-temperature profiles in the buffalo compute center. 
The X-axis is the time,  and the Y-axis gives two values: green line is the task execution time (CPU time), red line shows compute node temperature. 
Figure \ref{F:buffalo} clearly  indicates the task and temperature correlation: 
normally as compute loads in term of task CPU time augment,   compute node temperatures  increases incidentally. 
Their correlation coefficient is $0.65$. 
Figure \ref{F:crafty} shows a task-temperature profile, 
which is obtained  by running SPEC 2000 benchmark (crafty) on a IBM BladeCenter 
with 2 GB memory and Red Hat Enterprise Linux AS 3.

\FIGURE{!h}
 {images/figure10.png}
  {1.0}
 {Two types of tasks' task-temperature profiles}
 {F:t10}

Suppose there are 2 tasks, $j_1$ and $j_2$, with task-temperature profiles, $f(j_1)$, $f(j_2)$ shown in Figure \ref{F:t10}.Now $j1$ and $j_2$ are modeled as follows:

%\begin {equation}
$~~~~~~~~~~~~~~~~~~j_1=(0, 2, 20, f(j_1))$
%\end {equation}

Where, $j_1$ starts at 0 second, needs 2 processors; Its execution time is 20 seconds, and task-temperature profile is $f(j_1)$.

%\begin {equation}
$~~~~~~~~~~~~~~~~~~j_2=(0, 1, 40, f(j_2))$
%\end {equation}

Where, $j_2$ starts at 0 second, needs 1 processors; Its execution time is 40 seconds, and task-temperature profile is $f(j_2)$.

Suppose we have four identical computing nodes $(n_1,..., n_4)$, with current temperature: 

\begin{quote}
$n_1=40^\circ C$ \\
$n_2=32^\circ C$ \\
$n_3=34^\circ C$ \\
$n_4=32^\circ C$ \\
\end{quote}

The goal of our task scheduling is to minimize the temperature profile deviation of computing nodes after tasks are  executed, and try to minimize the maximal temperature of computing nodes (mathematically, these two objectives are the same). Now a thermal-aware optimized scheduling of $j_1, j_2$ is as follows:

\begin{quote}
$schedule_1$:\\
$\BH j_1\to n_2, n_4$\\
$\BH j_2 \to n_3$\\
\end{quote}

After task execution, the temperature of these nodes will become:

\begin{quote}
$t(n_1)=
t(n_2)=
t(n_3)=
t(n_4)=
40^\circ C$ \\
$Max~temperature=40^\circ C$ \\
$\sigma=0$ \\
\end{quote}

where $\sigma$ indicates the standard deviation. If the tasks are allocated with another schedule, for example: 
  
\begin{quote}
$schedule_2$: \\
$\BH j_1\to n_1, n_2$ \\
$\BH j_2 \to n_3$ \\
\end{quote}

After task execution, the temperature of these nodes will become:

\begin{quote}
$t(n_1)=48^\circ C$ \\
$t(n_2)=40^\circ C$ \\
$t(n_3)=40^\circ C$ \\ 
$t(n_4)=32^\circ C$ \\
$Max~temperature=48^\circ C$ \\
$\sigma=5.6$ \\
\end{quote}
%deviation  = 20^2+10^2+10^2 

Hence, according to our optimization criterion that is motivated by \cite{??} and \cite{??}, $schedule_1$ is better than $schedule_2$.

\begin{comment}
As presented in \cite{DBLP:conf/aPcsac/VandersterBD07}, differenttypes of tasks have different temperature profiles, i.e. different tasks result in different temperature increase on the same computing nodes. Based on the experiment result in \cite{DBLP:conf/aPcsac/VandersterBD07}, we present an example to illustrate our thermal-aware task scheduling ???\footnote{word missing} that is an essential part of our architectural design and aims to keep the temperature distribution in the data centers as even as possible. \GVL{why is this important, why would that reduce the cost. ...}  
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%% GOT TILL HERE NAD NEED TO SLEEP %%%%%%%%%%%%%

%%\FIGURE{htb}
%% {images/figure6.png}
%% {0.8}
%% {The relationship between Class a tasks' execution time and the temperature increase it caused. }
%% {F:t1}
%%\FIGURE{htb}
%% {images/figure7.png}
%% {0.8}
%%{The relationship between Class b tasks' execution time and the temperature increase it caused }
%% {F:t2}
%\begin{table}[htb]
%\caption{ Class A Tasks execution time and the temperature nodes}
%\label {T:t1}
%\begin{center}
%\begin{small}
%\begin {tabular} {|r|l|l|}
%\hline 
%Task execution time & 20 seconds &40 seconds\\
%\hline
% Temperature increase & 2.2 $^\circ$C &6.1 $^\circ$C \\
%\hline
%\end {tabular}
%\end{small}
%\end{center}

%\caption{ Class B Tasks execution time and the temperature nodes}
%\label {T:t2}
%\begin{center}
%\begin{small}
%\begin {tabular} {|r|l|l|}
%\hline 
%Task execution time & 20 seconds &40 seconds\\
%\hline
% Temperature increase & 8 $^\circ$C &10 $^\circ$C \\
%\hline
%\end {tabular}
%\end{small}
%\end{center}
%\end {table}

%To present a straight forward example let us assume that all tasks are part of two classes\footnote{it is not specified what a class is. it may be obvious from the paper, but the description here is highly confusing. I have taken the liberty to guess what you mean. it may be wrong and still incomplete, but at least its a start. the section as it was, was not understandable and actually wrong as we do not explain tables but tables are used to explain what we say in the text.} $A$, and $B$ that have some specific characteristics. These characteristics have been chosen based on 
%real application data obtained through experiments conducted on ... \footnote{this is what i mentioned before}.

%For tasks in the class $A$ let us assume the characteistics summarized in Table \ref{T:t1}, and for class $B$ the characteristics in table \ref{T:t2}. The tables specify a temperature increase given a particular execution time.\footnote{it is unclear if you do the same functionality in both tasks and if they only distingush itself through DVS. YOu realy have an incomplete example, and need to make sure you explain not the table, but the example and refer to the table.} 
%Let us focus on tasks in class $A$ and its temperature behaviour. Here, if the tasks is executed in 20 seconds, the temperature of the node which the task ran on will increase 2.2 $^\circ$C. However, if the same task is executed in 40 seconds, the node temperature increase is 6.1 $^\circ$C. 
% 
% \FIGURE{htb}
% {images/figure8.png}
% {0.8}
%{Thermal-aware task scheduling. }
% {F:t3}

%Let us assume there are two nodes in the data center with different current temperature values. Let us assume the temperatures are 
% there are three nodes in the data center with the temperature of 12 $^\circ$C, 8$^\circ$C and 10$^\circ$c respectively (Figure \ref {F:t3}). 
%Now we like to submit two nodes with the charcteristics specified in class $A$ and class $B$. Task a belongs to Class $B$ and it needs to run 20 seconds\footnote{contradiction to my understanding of the example}. So running task a will result in 8 $^\circ$C temperature increase. 

%According our thermal aware task scheduling to minimize ... \GVL{what}, we assign task a running on $n2$ which has the lowest temperature among 3 nodes. Task 2 belongs to Class a and it needs to run 20 seconds. So running task 2 will result in 2.2 $^\circ$C temperature increase. We assign task 2 to $n3$ to balance the temperature distribution in the data center.

%\GVL{ so what, this does not show anything and is more like a puzzle than an explanation. .. this example has not helped at all.}

%\GVL{maybe move on and make your other stuff and fix this entire section later.}

%%%%%%%%%%%%
\section{System Model}
%%%%%%%%%%%%%
This sections provides the formal description of data center, cluster, node and task models, which are employed as basis of the formal problem definition in Section \ref{S:problem} and the scheduling algorithm in Section \ref{S:algorithm}.

We model the data center $\Gamma$ as: 

\begin{equation}
\Gamma=\{R\} ~and~ R=\bigcup_{j=1}^{N} \{rack_j\} ~and~ rack_j=\bigcup_{i=1}^{M}\{n_i^j\}
\end{equation}

Where, $R$ indicates the racks in the data center. 
$R$ is composed of $N$ racks; $rack_j$ is the j$^{th}$ rack of $R$. 
and $rack_j$ is composed of $M$ nodes; $n_i^j$ is the i$^{th}$ node in the $rack_j$.

We model the node as:

\begin{equation}
n_i^j=(speed, temp,\bigtriangleup{temp},duration)
\end{equation}

Where, the node $node_i^j$ has processors with processing performance of $speed$ MIPS (Million Instruction Per Second); $temp$ is the node's current temperature; $\bigtriangleup{temp}$ \GVL{this shoudl realy not be in node as it is independent!!!!} indicates the expected temperature increase on the node due to the scheduled tasks. $duration$ indicates the execution time of scheduled tasks.\GVL{duration also does not realy belong to n}

We model the tasks to be scheduled on our data center:

\begin{equation}
J=\bigcup_{k=1}^{L} \{j_k\}
\end{equation}

\begin{equation}
j_k=(t_{arrive}, num, walltime, f_{temp})
\end{equation}

Where, there are $L$ tasks submitted and to be scheduled. $j_k$ is the k$th$ task which comes 
at $t_{arrive}$; The task needs $num$ processors and lasts $walltime$; $f_{temp}$ is a function that can estimate nodes' temperature rise caused by the execution of the task. The function takes the task's $walltime$ as input and outputs the estimated temperature rise. 

%%%%%%%%%%%%
\section{Problem Definition}\label{S:problem}
%%%%%%%%%%%%%%%%%

Mathematically scheduling the tasks to the computing nodes is a function, $f$, which maps the tasks to the computing nodes.

\begin{equation}
t_{start}=f:(j_a) \rightarrow (node_{bc}) 
\end{equation}

\GVL{this formula is clearly wrong as the = sign is used in a non standard fashion, t is a parameter ...}
Where, $j_a$ is arbitrary task submitted to data centers; It is scheduled to run on $node_{bc}$ at the time $t_{start}$. Now we define the problem of thermal-aware scheduling as follows: 

Given a set of tasks. Find an optimal schedule to assign each task to the computing nodes to minimize $SD$, the computing nodes' temperature standard deviation.

\begin{equation}
AVE=\frac{\sum_{k=1}^{L} (j_k.temp)}{L}
\end{equation}

\begin{equation}
SD=\sqrt{\frac{\sum_{k=1}^{L} (j_k.temp-AVE)^2}{L}}
\end{equation}

Where, $AVE$ is the average temperature of computing nodes.
%---------------------------------------------------------------------
\section{Thermal-Aware Task Scheduling Algorithm}\label{S:algorithm}
%---------------------------------------------------------------------

\begin{algorithm}
\caption{Scheduling tasks on nodes}
\label{A:1}
~\\
1:    FOR $i =1$  TO  $N$ DO\\
2:\BH FOR $j =1$  TO  $M$ DO\\
3: \BH\BH   $node_{ij}.temp$=0\\
4: \BH\BH   $node_{ij}.\bigtriangleup{temp}$=0\\
5: \BH\BH   $node_{ij}. duration $=0\\
6:  \BH END FOR\\
7:  END FOR\\
\\ 
8:       $t=0$\\
9:       WHILE ($\lnot$ finished) DO\\
10:\BH     Schedule the set of upcoming tasks with Algorithm \ref{A:2}\\
11:\BH     $t=t+interval$\\
12:\BH      FOR $i =1$  TO  $N$ DO\\
13:\BH \BH FOR $j =1$  TO  $M$ DO\\
14:\BH\BH\BH    WITH $node_{ij}$\\
14: \BH\BH\BH   $.temp=+.\bigtriangleup{temp}* interval/. duration $\\
15: \BH\BH\BH   $.\bigtriangleup{temp}=-.\bigtriangleup{temp}* interval/.duration$\\
16: \BH\BH  \BH $.duration=-interval $\\
14:\BH\BH\BH  END WITH\\
18:  \BH\BH END FOR\\
19:  \BH END FOR\\
20:			 END WHILE\\
\end{algorithm}
%In this section, we present our thermal aware task scheduling algorithm. Assume all of the nodes in data centers are homogeneous and we can estimate the type of tasks and tasks' wall time. Obviously, upcoming tasks generate equal temperature increases no matter how tasks are allocated to nodes. The goal of thermal aware task scheduling algorithm is to keep temperature distribution among nodes as even as possible so that less cooling energy is consumed to prevent nodes' temperature reaching its redline. 
Algorithm \ref{A:1} shows thermal-aware task scheduling process. The scheduling algorithm runs as a daemon in a data center with a predefined scheduling interval, $interval$. First the scheduler initiates each node as follows (line 1-7)
\begin{itemize}
\item Set each node's initial temperature as 0;
\item Set no task running on the nodes;
\end{itemize}
Then the scheduler iterates scheduling tasks (line 9-20) with predefined interval value, $interval$ from starting time 0; In each scheduling round, the scheduler places upcoming tasks in the queue to the nodes with Algorithm \ref{A:2}. During the period of scheduling interval, incoming tasks arrive at the scheduler and will be scheduled at the next schedule round. At the same time, the scheduled tasks run on nodes and causes nodes' temperature increase. 
 
\begin{algorithm}
\caption{One-Round Scheduling Algorithm }
\label{A:2}
~\\
1:       Sort nodes in the ascending order of their temperature\\
2:       Sort tasks in the ascending order of their causing temperature rise\\
3:       FOR $k=1$  TO  $L$ DO\\
4: \BH         FOR $nn=1$ TO $j_k.num$ DO\\ 
5:\BH \BH    FOR $i =1$  TO  $N$ DO\\
6:\BH\BH\BH FOR $j =1$  TO  $M$ DO\\
7:\BH\BH\BH\BH  $node_{ij}.\bigtriangleup{temp}=j_k.f_{temp}$\\
8:\BH\BH\BH \BH $node_{ij}.duration=j_k.walltime$\\
9:\BH\BH\BH END FOR\\
10:\BH \BH    END FOR\\
11: \BH        END FOR\\
12:     END FOR\\          
\end{algorithm}

In Algorithm \ref{A:2}, the scheduler sorts the nodes in ascending order of nodes' temperature and tasks in ascending order of the temperature rise they causes. Then the scheduler allocates each tasks on the computing nodes sequently. 
%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Simulation Results}
%%%%%%%%%%%%%%%%%%%%%%%%
%In this section, we present simulation results of proposed thermal-aware
%task scheduling. In the simulation, we have 10 nodes. 

%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
%%%%%%%%%%%%%%%%%%%%%%%
To manage computing resources in a more energy efficient fashion, we present our thermal-aware resource management framework which has a layered architecture and can address a series of resource management related problems. Based on our study on different types of task-temperature profiles and the relationship between temperature distribution and cooling energy consumption, we also developed a thermal-aware task scheduling as part of our framework with a goal of conserving cooling energy cost and increasing system reliability. 

In the future, We would further our study and investigate other thermal characteristic of data centers. In addition, we would continue the development and improvement of our thermal-aware resource management framework.